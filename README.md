# 视觉作业：教电脑在图片和视频里找到足球！

- **课程**：仿人足球机器人技术综合实践（2025 年秋季学期）
- **截止日期**：2025 年 11 月 15 日 23:59

---

## 一、实验指南

### 你的任务是什么？

简单来说，你要编写代码，教会计算机自动识别不同场景下的足球，并得到它的位置和半径 `(x, y, r)`。

你需要至少实现一个传统方法和一个机器学习方法

1. **传统方法**：利用足球和周围环境在颜色、形状上的区别来寻找。
2. **机器学习方法**：借助已经训练好的模型直接进行目标检测。

我们已经在`detectors.py`提供了一些示例方法，你可以参考它们来实现你的检测器。请注意，你不能直接使用以下这些示例方法来直接完成作业，你需要进行修改和创新。

- `rgb_mbr_image_detector`: 在RGB色彩空间上检测白色像素，然后通过最小外接矩形定位足球。
- `faster_rcnn_image_detector`: 使用预训练的 Faster R-CNN ResNet50 FPN 模型进行目标检测，并筛选出类别为“sports ball”的检测结果。

### 我们提供的代码框架

我们已经为你搭好了一个基础框架，你只需要在指定的地方填写核心代码。

- `main.py`: **主程序**。它会自动加载你的数据，并调用你写的检测器去工作，最后保存结果。**无需修改**。
- `detectors.py`: **检测器**。你将在这里编写和你的检测器逻辑。**你只需要修改这个文件**。
- `assets/`: **待处理文件**。我们提供了8张待处理图片（`1.png`~`8.png`）和2段待处理视频（`9.mp4`、`10.mp4`）。**请勿修改**。
- `outputs/`: **程序输出**。程序运行后，带红圈的结果会自动保存在这里。

#### 检测器逻辑接口规范

为了让你的检测器能和主程序 `main.py` 顺利沟通，你需要遵守以下约定：

对于图片检测器，应当满足以下要求：

```python
@register_image_detector
def dummy_image_detector(image: Image) -> list[Detection]: ...
```

- **输入**：PIL 格式的图片对象 `image`。
- **输出**：一个 `Detection` 对象列表，每个对象包含足球的 `(x, y, r)` 信息。

对于视频检测器，应当满足以下要求：

```python
@register_video_detector
def dummy_video_detector(frames: list[Image]) -> list[list[Detection]]: ...
```

- **输入**：PIL 格式的图片对象列表 `frames`，每个对象对应视频中的一帧。
- **输出**：一个二维列表。外层列表对应每一帧，内层列表包含该帧中所有检测到的足球的 `Detection` 对象。
  - 返回的外层列表长度必须与输入帧 `len(frames)` 完全一致；每一帧都需对应一份检测结果（可为空列表）。

我们建议你直接复制`dummy_image_detector`或`dummy_video_detector`的函数签名，修改为你自己的函数名即可。请不要忘记复制上方的装饰器 `@register_image_detector` 或 `@register_video_detector`，否则主程序将无法识别你的检测器！

为避免示例方法干扰你的结果输出，建议在提交前仅保留希望参与评分的方法的注册（即确保 `detectors.py` 中只有你的方法带有 `@register_*` 装饰器）。

### 快速上手指南

在开始之前，你需要安装 Miniconda。请运行以下命令下载安装包：

```bash
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py312_24.11.1-0-Linux-x86_64.sh
```

下载完毕后，运行安装脚本：

```bash
bash ~/Miniconda3-py312_24.11.1-0-Linux-x86_64.sh -u
```

按照提示进行安装，你可能需要输入`y`、`yes`或回车。当你阅读用户条例的时候，可以按一次`q`跳过。

关闭终端并重新打开。然后，运行以下命令添加清华源：

```bash
cat <<EOF > ~/.condarc
channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
EOF
```

添加完毕后，创建并激活一个新的 Conda 环境。在这里，我们将环境命名为 `pd-hsr-assignment-4`：

```bash
conda create -n pd-hsr-assignment-4 -y python=3.12
```

Miniconda 环境配置完毕后，你可以开始运行本代码框架了。首先，请确保已经解压所有文件到同一目录下。你的文件结构应该是这样的：

```
.
├── assets/
│   ├── 1.png
│   ├── 2.png
│   ├── 3.png
│   ├── 4.png
│   ├── 5.png
│   ├── 6.png
│   ├── 7.png
│   ├── 8.png
│   ├── 9.mp4
│   └── 10.mp4
├── detectors.py
├── main.py
├── pyproject.toml
└── README.pdf
```

切换到解压的所有文件所在的目录后，激活你刚才创建的 Conda 环境：

```bash
conda activate pd-hsr-assignment-4
```

随后，安装所需依赖。

```bash
pip install -e .
```

我们提供了一些示例代码，你可以试着直接运行它们，看看效果如何（首次运行会自动下载模型权重，需联网）：

```bash
python main.py
```

输出结果会保存在 `outputs/` 文件夹中。

接下来，你可以开始编写你的检测器逻辑了！请参考上文的“检测器逻辑接口规范”部分，修改 `detectors.py` 文件，完成你的基于传统方法和机器学习方法的检测器。注意：不得直接使用我们提供的示例方法代码，必须有自己的修改和创新。

每当你修改完代码后，可以再次运行上面的命令来测试效果。

> 运行位置要求：请从仓库根目录运行 `python main.py`，以保证 `./assets` 与 `./outputs` 等相对路径正确。

---

## 二、报告与提交

### 成果打包清单

请将以下所有内容打包成一个 `.zip` 文件提交：

1.  **你的代码**：特别是你修改过的 `detectors.py`。
2.  **结果展示**：`outputs/` 
3.  **一份报告 (PDF)**：分享你的实验过程和结果。

你的压缩包结构应类似于：

```
.
├── outputs/
│   ├── your_custom_detector_1.png
│   └── ... (其他输出文件)
├── detectors.py （关键：你修改过的版本）
├── main.py
└── report.pdf （关键：你的报告）
```

请把压缩包命名为你的学号，例如 `2025010000.zip`。

### 如何写报告？

别担心，不需要长篇大论！把它当成一份实验记录，主要目的是展示你的尝试和思考。建议包含：

1.  **摘要**：一句话总结你做了什么，结果怎么样。
2.  **我的方法**：
    - **方法X（经典方法）**：你是怎么选取颜色范围/形状特征的？用了哪些技巧？（可以说“我试了几个颜色范围，发现这个效果最好”）
    - **方法Y（机器学习方法）**：你用了哪个模型？有没有调整什么参数（比如置信度阈值）？
    - **更多的方法**（如果有的话）。
4.  **结果分析**：
    - **成功案例**：贴几张成功检测的截图。
    - **失败案例**：也贴几张失败的截图，并猜猜为什么会失败（例如：“这个场景光线太暗，颜色法失效了”）。
    - **性能分析**：运行时间、资源消耗等。
    - **对比**：各种方法在哪些场景里表现更好？
6.  **实际场景应用建议**：结合你的观察，谈谈你觉得在Webots场景中，哪种方法更适合用来检测足球，为什么？
7.  **AI 工具使用声明**：见下文。

### 学术诚信与 AI 使用声明

请在报告中增加“生成式人工智能工具使用声明”：

-   **若未使用**生成式 AI 工具，请写：“本人郑重声明，本次作业所有内容均为独立完成，未借助生成式人工智能工具。”
-   **若使用**了生成式 AI 工具，请说明：
    -   **使用了哪些工具**：例如 Copilot、ChatGPT、Claude 等。
    -   **用于哪些环节**：例如资料检索、代码补全、调试建议、文案润色等。
    -   **你个人进行了哪些独立工作与判断**，以确保内容的原创性与正确性。

严禁直接抄袭他人代码/文字，确保可解释与可复现。

---

## 三、评分标准

- 代码实现（40分）
  - 传统方法（20分）
  - 机器学习方法（20分）
  - 其他方法（如果有的话）
- 实验报告（40分）
  - 方法描述与创新（20分）
    - 传统方法（10分）
    - 机器学习方法（10分）
    - 其他方法（如果有的话）
  - 结果分析与应用建议（10分）
  - AI 工具使用声明（10分）
- 输出结果（20分）
  - 传统方法（10分）
  - 机器学习方法（10分）
  - 其他方法（如果有的话）（每种5分）

对传统方法的输出结果，按 8 个图片和 2 个视频的检测效果进行评分，每个图片/视频 2 分，但满分不超过 10 分；对机器学习方法的输出结果，按 `6.png` ~ `8.png` 和 2 个视频的检测效果进行评分，每个图片/视频 2 分，满分 10 分。

若实现了更多的方法，每多实现一种**代码实现结构不同**的方法可获得加分。判定示例：
- 传统方法之间：例如“RGB阈值+最小外接矩形” 与 “霍夫圆变换” 视为结构不同；
- 学习方法之间：例如“torchvision Faster R-CNN” 与 “Ultralytics YOLOv8/RT-DETR/DETR” 视为结构不同；
- 不计作结构不同：在同一库内仅替换同类模型/权重（如 Faster R-CNN 只换 backbone）。
加分按表现，每个图片/视频 0.5 分，每种方法最高加 5 分。

评分收集口径：请在提交前确认 `detectors.py` 中仅保留你希望评分的方法（即带有 `@register_*` 的函数）。示例方法如保留注册，可能会被主程序一并运行，但不计入评分。

原创性说明：不得直接使用示例方法，必须有实质修改与创新。请在报告中明确列出你的方法与示例的主要差异（例如阈值设计、特征工程、后处理、模型与参数选择等）。

总分不超过 100 分。
